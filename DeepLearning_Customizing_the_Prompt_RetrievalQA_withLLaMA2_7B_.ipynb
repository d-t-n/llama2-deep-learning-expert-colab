{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RRYSu48huSUW"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain huggingface_hub tiktoken\n",
        "!pip -q install chromadb\n",
        "!pip -q install PyPDF2 pypdf sentence_transformers\n",
        "!pip -q install --upgrade together\n",
        "\n",
        "!pip -q install -U FlagEmbedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvIjaK53dP5l"
      },
      "source": [
        "## RetrievalQA with LLaMA 2-7B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNA4TsHpu6OM",
        "outputId": "d0cfc58f-552c-4033-f3d0-6dcdf7476be1"
      },
      "outputs": [],
      "source": [
        "!pip -q install git+https://github.com/huggingface/transformers # need to install from github\n",
        "!pip install -q datasets loralib sentencepiece\n",
        "!pip -q install bitsandbytes accelerate xformers einops\n",
        "!pip -q install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-KFB7J_u_3L",
        "outputId": "7b90b2fb-26dc-40ba-88ba-8733bf39a2e6"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34,
          "referenced_widgets": [
            "b06a82f09ca745b48c61f55eb9008b38"
          ]
        },
        "id": "gP-055tSQ1wd",
        "outputId": "371d6e02-9d9e-4b86-9858-400858eef5df"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LgkNP4QKQ7NS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "affb8c03e7da488a8dbdd407e291a4c2",
            "0bb3f8f2c74a44e296ef4c1e60c2d179",
            "466a2d6eceae45598273013346d250a1",
            "a8f4c1f7381a4fd591b61b2e69490b3c",
            "8c13c97307fa4d34900c16091ec7876e",
            "2ed179d14eac44f3a9195ebeab83fc58",
            "1e7601f17d26426ab0853647ce9459ec",
            "33d9c748c55e46af8c04af49548ad71c",
            "87cef16ac75846cf82f3e9b4dc22167a",
            "bd4447ef3def4c70aa9eb136a6873a20",
            "0d0f73d136dc4529b3a10331e0fc8d30"
          ]
        },
        "id": "UBHJzBnNR3Po",
        "outputId": "ec7e6dd1-bb3f-46e5-a25c-df37f3d1254b"
      },
      "outputs": [],
      "source": [
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "                                          use_auth_token=True,)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "                                             device_map='auto',\n",
        "                                             torch_dtype=torch.float16,\n",
        "                                             use_auth_token=True,\n",
        "                                            #  load_in_8bit=True,\n",
        "                                            #  load_in_4bit=True\n",
        "                                             )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlQzln_PRonn",
        "outputId": "9e20b23b-833f-49c7-dea7-c6c44485daf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-08-29 15:31:48--  http://imlab.postech.ac.kr/dkim/class/csed514_2019s/DeepLearningBook.pdf\n",
            "Resolving imlab.postech.ac.kr (imlab.postech.ac.kr)... 141.223.12.103\n",
            "Connecting to imlab.postech.ac.kr (imlab.postech.ac.kr)|141.223.12.103|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21642066 (21M) [application/pdf]\n",
            "Saving to: ‘DeepLearningBook.pdf’\n",
            "\n",
            "DeepLearningBook.pd 100%[===================>]  20,64M  4,55MB/s    in 4,5s    \n",
            "\n",
            "2023-08-29 15:31:53 (4,55 MB/s) - ‘DeepLearningBook.pdf’ saved [21642066/21642066]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!wget http://imlab.postech.ac.kr/dkim/class/csed514_2019s/DeepLearningBook.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AnZQpL_IZZZ"
      },
      "source": [
        "# LangChain multi-doc retriever with ChromaDB\n",
        "\n",
        "***Key Points***\n",
        "- Multiple Files - PDFs\n",
        "- ChromaDB\n",
        "- LLaMA-2 LLM\n",
        "- BGE Embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgfdhZ5uRpFn"
      },
      "source": [
        "## Setting up LangChain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Y_2-HBI3RpFn"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XHVE9uFb3Ajj"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UcQKUId3X2M"
      },
      "source": [
        "## Load multiple and process documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PRSeXXc_3Ypj"
      },
      "outputs": [],
      "source": [
        "# Load and process the text files\n",
        "# loader = TextLoader('single_text_file.txt')\n",
        "loader = DirectoryLoader('./', glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
        "\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT6KgAIT_BtB",
        "outputId": "06c2a736-0ccb-4e19-e05e-aa0de020e9f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "802"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3__nT0D4Fkmg",
        "outputId": "09542fbc-4dcd-4bfc-db59-e366d51b1bd8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2410"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#splitting the text into\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "len(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mi9VSazP0RSN",
        "outputId": "98c0e7a9-0c41-4b7e-f38d-a1619ae45769"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content='CONTENTS3.2RandomVariables. . . . .. . . . . . . .. . . . . . . . .. . . .563.3ProbabilityDistributions. . . . . . . . .. . . . . . . .. . . . . .563.4MarginalProbability. . . . . . . . .. . . . . . . . .. . . . . . .583.5ConditionalProbability. .. . . . . . . .. . . . . . . .. . . . .593.6TheChainRuleofConditionalProbabilities. . . . . . . . .. . .593.7IndependenceandConditionalIndependence. . . . . . . . .. . .603.8Expectation,VarianceandCovariance. . . . . . . . . .. . . . .603.9CommonProbabilityDistributions. . . . . . . . . . . . . . .. .623.10UsefulPropertiesofCommonFunctions. . .. . . . . . . . .. .673.11Bayes’Rule. . . . . . . . . .. . . . . . . .. . . . . . . .. . . .703.12TechnicalDetailsofContinuousVariables. . . . . .. . . . . . .713.13InformationTheory. . . . . . . . . .. . . . . . . .. . . . . . . .723.14StructuredProbabilisticModels. . . .. . . . . . . .. . . . . . .754NumericalComputation804.1OverﬂowandUnderﬂow. . . . . . . . .. . . . . . . .. . . . .', metadata={'source': 'DeepLearningBook.pdf', 'page': 2})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts[3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhs0C0FYASlM"
      },
      "source": [
        "## HF BGE Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Emj46ATxtV9C"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "model_name = \"BAAI/bge-base-en\"\n",
        "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
        "\n",
        "model_norm = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs={'device': 'cuda'},\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ItmW2tygHHy",
        "outputId": "9cda72a1-591c-49e9-b5dc-581a225d8a10"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xaxNlMtgV8E",
        "outputId": "3fa0ee1e-5a22-44e6-98da-1b8f017646c3"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsYsIy8F4cdm"
      },
      "source": [
        "## create the DB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "Q_eTIZwf4Dk2",
        "outputId": "43f67d21-f787-4308-b8da-77e6044b2e73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 8s, sys: 866 ms, total: 1min 9s\n",
            "Wall time: 57.3 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Embed and store the texts\n",
        "# Supplying a persist_directory will store the embeddings on disk\n",
        "\n",
        "persist_directory = 'db'\n",
        "\n",
        "## Here is the nmew embeddings being used\n",
        "embedding = model_norm\n",
        "\n",
        "vectordb = Chroma.from_documents(documents=texts,\n",
        "                                 embedding=embedding,\n",
        "                                 persist_directory=persist_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siLXR-XT0JoI"
      },
      "source": [
        "## Make a retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "jVWgPJXs1yRq",
        "outputId": "5c99ca81-9c1f-4c35-d370-14d8e8d793f3"
      },
      "outputs": [],
      "source": [
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ia-4OXa5IeP"
      },
      "source": [
        "## Make a chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "IrVIuygNuBVT"
      },
      "outputs": [],
      "source": [
        "## Default LLaMA-2 prompt style\n",
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
        "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
        "\n",
        "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
        "\n",
        "def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
        "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
        "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
        "    return prompt_template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "vVKwoAnpuD79"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible using the context text provided. Your answers should only answer the question once and not have any text after the answer is done.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \\n<</SYS>>\\n\\nCONTEXT:/n/n {context}/n\\n\\nQuestion: {question}[/INST]\""
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sys_prompt = \"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible using the context text provided. Your answers should only answer the question once and not have any text after the answer is done.\n",
        "\n",
        "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \"\"\"\n",
        "\n",
        "instruction = \"\"\"CONTEXT:/n/n {context}/n\n",
        "\n",
        "Question: {question}\"\"\"\n",
        "get_prompt(instruction, sys_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "p3Tqn50wcVaS"
      },
      "outputs": [],
      "source": [
        "# Use a pipeline for later\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\",\n",
        "                model=model,\n",
        "                tokenizer= tokenizer,\n",
        "                torch_dtype=torch.bfloat16,\n",
        "                device_map=\"auto\",\n",
        "                max_new_tokens = 512,\n",
        "                do_sample=True,\n",
        "                top_k=30,\n",
        "                num_return_sequences=1,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain import HuggingFacePipeline\n",
        "from langchain import PromptTemplate,  LLMChain\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "dCtX_DK9S-K0"
      },
      "outputs": [],
      "source": [
        "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1LmgdDIfsjXp"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "prompt_template = get_prompt(instruction, sys_prompt)\n",
        "\n",
        "llama_prompt = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ts3sZb7fsoMN"
      },
      "outputs": [],
      "source": [
        "chain_type_kwargs = {\"prompt\": llama_prompt}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "MGx8XblM4shW"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import prompt\n",
        "# create the chain to answer questions\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                       chain_type=\"stuff\",\n",
        "                                       retriever=retriever,\n",
        "                                       chain_type_kwargs=chain_type_kwargs,\n",
        "                                       return_source_documents=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "LZEo26mw8e5k"
      },
      "outputs": [],
      "source": [
        "## Cite sources\n",
        "\n",
        "import textwrap\n",
        "\n",
        "def wrap_text_preserve_newlines(text, width=110):\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text\n",
        "\n",
        "def process_llm_response(llm_response):\n",
        "    print(wrap_text_preserve_newlines(llm_response['result']))\n",
        "    print('\\n\\nSources:')\n",
        "    for source in llm_response[\"source_documents\"]:\n",
        "        print(source.metadata['source'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "wKfX4vX-5RFT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  A CNN (Convolutional Neural Network) is a type of neural network that is particularly well-suited for image\n",
            "classification tasks. It uses convolutional layers to extract features from images, followed by pooling layers\n",
            "to reduce the dimensionality of the feature space.\n",
            "A RNN (Recurrent Neural Network) is a type of neural network that is well-suited for sequential data, such as\n",
            "text or time series data. It has a feedback loop that allows information from previous time steps to influence\n",
            "the current step, allowing it to capture long-term dependencies in the data.\n",
            "An LSTM (Long Short-Term Memory) is a type of RNN that is designed to handle the vanishing gradient problem\n",
            "that can occur in traditional RNNs. It uses a special type of cell state that allows it to selectively forget\n",
            "or remember information from previous time steps, allowing it to learn long-term dependencies in the data.\n",
            "\n",
            "\n",
            "Sources:\n",
            "new_papers/new_papers/Flash-attention.pdf\n",
            "new_papers/new_papers/Flash-attention.pdf\n",
            "DeepLearningBook.pdf\n",
            "new_papers/new_papers/Flash-attention.pdf\n",
            "DeepLearningBook.pdf\n"
          ]
        }
      ],
      "source": [
        "# full example\n",
        "query = \"What is a CNN, a RNN and a LSTM?\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "IPIhZWAR5n3X"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('similarity', <langchain.vectorstores.chroma.Chroma at 0x7f8c697878d0>)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qa_chain.retriever.search_type , qa_chain.retriever.vectorstore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "3_lp0_796P_-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INST]<<SYS>>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible using the context text provided. Your answers should only answer the question once and not have any text after the answer is done.\n",
            "\n",
            "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \n",
            "<</SYS>>\n",
            "\n",
            "CONTEXT:/n/n {context}/n\n",
            "\n",
            "Question: {question}[/INST]\n"
          ]
        }
      ],
      "source": [
        "print(qa_chain.combine_documents_chain.llm_chain.prompt.template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65Bmvfjk9MOf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0bb3f8f2c74a44e296ef4c1e60c2d179": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ed179d14eac44f3a9195ebeab83fc58",
            "placeholder": "​",
            "style": "IPY_MODEL_1e7601f17d26426ab0853647ce9459ec",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "0d0f73d136dc4529b3a10331e0fc8d30": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e7601f17d26426ab0853647ce9459ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ed179d14eac44f3a9195ebeab83fc58": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33d9c748c55e46af8c04af49548ad71c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "466a2d6eceae45598273013346d250a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33d9c748c55e46af8c04af49548ad71c",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_87cef16ac75846cf82f3e9b4dc22167a",
            "value": 2
          }
        },
        "87cef16ac75846cf82f3e9b4dc22167a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8c13c97307fa4d34900c16091ec7876e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8f4c1f7381a4fd591b61b2e69490b3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd4447ef3def4c70aa9eb136a6873a20",
            "placeholder": "​",
            "style": "IPY_MODEL_0d0f73d136dc4529b3a10331e0fc8d30",
            "value": " 2/2 [01:08&lt;00:00, 31.03s/it]"
          }
        },
        "affb8c03e7da488a8dbdd407e291a4c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0bb3f8f2c74a44e296ef4c1e60c2d179",
              "IPY_MODEL_466a2d6eceae45598273013346d250a1",
              "IPY_MODEL_a8f4c1f7381a4fd591b61b2e69490b3c"
            ],
            "layout": "IPY_MODEL_8c13c97307fa4d34900c16091ec7876e"
          }
        },
        "bd4447ef3def4c70aa9eb136a6873a20": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
